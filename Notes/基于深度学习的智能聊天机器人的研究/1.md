采用word2vec 
针对长距离依赖问题, 配合使用了Attention model 和 Bi-LTSM(外部记忆单元). 其中Attention model 解决了传统ENcoder-Decoder框架只有一个中间语义向量从而导致信息丢失, 信息沉余问题. LSTM模型通过学习长期依赖信息, 解决了循环神经网络 RNN的GRADIEnt Vanish(梯度消失)问题和模型训练中的长期依赖问题. Bi-LSTM模型解决了LSTM模型没有考虑到的下文的信息, 丢失了部分语意信息的问题.